#!/usr/bin/env python3

import os
import argparse
import subprocess as sp
import textwrap
import shutil
import uuid


parser = argparse.ArgumentParser(description=textwrap.dedent("""
    Start a new SafeLife training run.

    This performs the following steps:

    1. Clone the repo to a new temporary folder.
    2. Copy all source files over to the remote machine using rsync.
    3. ssh into the remote machine and
        a. create an alias to the soon-to-be-created data folder;
        b. start a tmux session that shares the training job's name;
        c. listens on the appropriate local port for tensorboard updates;
        d. starts training via the `start-training` script.

    Note that the `start-training` script will shut down the remote instance
    with a 10 minute lag before the script exits, whether via error or normal
    completion. The shutdown is there to prevent machine from idling and
    running up large bills, while the lag is designed so that it's possible
    to abort the shutdown by sshing into the remote machine and running
    `sudo shutdown -c`. This comes in handy when the script fails at startup
    due to user error or a bug.
    """), formatter_class=argparse.RawDescriptionHelpFormatter)
parser.add_argument('instance_name', help="name of the gcloud instance")
parser.add_argument('job_name', help="a unique name for this training job")
parser.add_argument('--port', default='6006',
    help="local port used to monitor tensorboard")
parser.add_argument('--commit-id',
    help="If supplied, the job is run from the specified commit id (or tag or "
    "branch). Otherwise, changes in the working tree are stashed and the "
    "job is run from the stashed commit. That is, the job is run using the "
    "code as it currently exists, minus any untracked files.")
parser.add_argument('--tag', nargs='?', const="auto",
    help="If set, the commit is given the specified tag. "
    "If auto, the tag is autogenerated with the form 'job-[job_name]'.")
args, remaining_args = parser.parse_known_args()

src_dir = '~/' + args.job_name
data_dir = '~/{job_name}/data/{job_name}/'.format(job_name=args.job_name)

quiet = {'stdout': sp.DEVNULL, 'stderr': sp.DEVNULL}

# There are a couple files which we typically want to keep out of git(hub),
# but we do want to add them to the remote machine if they're available.
extra_files = [
    'run-info.md',
    'wandb',
]

if args.commit_id:
    commit_id = args.commit_id
else:
    sp.run('; '.join([f"git add {f}" for f in extra_files]),
        shell=True, **quiet)
    result = sp.run("git stash create", shell=True, stdout=sp.PIPE, check=True)
    sp.run('; '.join([f"git restore --staged {f}" for f in extra_files]),
        shell=True, **quiet)
    commit_id = result.stdout.decode()
    if not commit_id:
        # means there were no stashed chages. Use HEAD instead.
        result = sp.run("git rev-parse HEAD", shell=True, stdout=sp.PIPE)
        commit_id = result.stdout.decode()

print(f"Setting job to commit id '{commit_id}'...")

if args.tag == 'auto':
    git_tag = f"job-{args.job_name}"
elif args.tag:
    git_tag = args.tag
else:
    # create a temporary tag
    git_tag = f'job-{uuid.uuid4()}'
result = sp.run(f"git tag {git_tag} {commit_id}", shell=True, **quiet)
if result.returncode == 128:
    print(f"Tag '{git_tag}' already exists. Aborting.")
    exit(1)
elif result.returncode != 0:
    print(f"Failed to add tag '{git_tag}'.")
    exit(1)
else:
    print(f"Adding tag '{git_tag}'...")

# Copy over the data
safety_dir = os.path.abspath(os.path.join(__file__, '../../'))
ssh_cmd = os.path.abspath(os.path.join(__file__, '../ssh'))

print("Cloning temporary repo...")
result = sp.run(
    f"git clone --no-local --single-branch --branch {git_tag} . ./tmp-repo",
    shell=True, capture_output=True)
if not args.tag:
    print("Removing temporary tag...")
    sp.run(f"git tag -d {git_tag}", shell=True, **quiet)
if result.returncode != 0:
    print("Error cloning the repo.")
    print(result.stdout)
    print(result.stderr)
    exit(1)

print("Syncing repo to the cloud...")
result = sp.run([
    'rsync', '--rsh', ssh_cmd, '-ra',
    './tmp-repo/', args.instance_name + ':' + src_dir])
shutil.rmtree("./tmp-repo")
if result.returncode != 0:
    exit(1)

print("Starting job...")
# Run the script, tunneling tensorboard to the specified port.
result = sp.run([
    ssh_cmd, args.instance_name, '-L', args.port + ':localhost:6006',

    # Set the path variable to include conda, which contains the latest
    # versions of python, torch, etc.
    # This is very specific to our particular gcloud setup.
    "PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:$PATH; "

    # Create a "current_job" directory. This is handy for re-entering the job.
    f"ln -nsf {src_dir} ~/current_job; "

    # Install dependencies.
    "sudo apt-get install ffmpeg --yes; "
    f"pip install -r {src_dir}/requirements.txt; "

    # And run the training script!
    # This uses tmux to prevent it from dying on hangup.
    # Note that if this session is already running, tmux should prevent us
    # from running it again.
    f"tmux new-session -s {args.job_name} "
    f"{src_dir}/start-training {data_dir} --ensure-gpu " + ' '.join(remaining_args)
    + " ; bash "
])
